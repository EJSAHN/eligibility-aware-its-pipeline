├─ its_pipeline_shareable.py
├─ data/
│  ├─ primer_catalog.csv        # required: columns = name,seq,dir
│  ├─ Mperniciosa.fasta
│  ├─ Mroreri.fasta
│  ├─ Pmegakarya/               # or Pmegakarya.fasta
│  └─ Ppalmivora/               # or Ppalmivora.fasta
└─ results/                     # created automatically

numpy==1.26.4
pandas==2.2.2
matplotlib==3.8.4
biopython==1.81
logomaker==0.8 

results/
*.pyc
__pycache__/
.ipynb_checkpoints/

# Eligibility‑aware ITS pipeline

## Install
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux: source .venv/bin/activate
pip install -r requirements.txt

## Run
# From the project root:
python its_pipeline_shareable.py

Outputs land in `results/`:
- calls_all_pairs.csv
- pair_catalog_auto.csv
- duplicate_report_by_pair.csv
- Ineligibility_breakdown.csv
- Table_all_pairs_by_species.csv
- Fig1_all_pairs_by_species_FIXED.pdf
- Fig2_*_logo.{pdf,png}, Fig2_*_entropy.{pdf,png}
- Fig3_Ppalmivora_ITS6_rarefaction.{pdf,png,csv}
- Delta_coverage_by_species_pair.csv
- FigS_DeltaCoverage.pdf (if enabled)
- Table_S3_sensitivity.csv
- FigS_Sensitivity.pdf (if enabled)

## Data layout
Put `primer_catalog.csv` and FASTA files/folders under `data/`.
Required primer names: ITS1, ITS2, ITS1F, ITS2_KYO2, ITS5, ITS4, fITS7, ITS86F, ITS3_KYO2, ITS6 (as used by the script).


# its_pipeline_shareable.py
# End-to-end ITS in-silico pipeline (eligibility-aware) with duplicate prevention
# + Δ-coverage table + sensitivity analysis (≤3 mismatches; allow 3' mismatch).
#
# Outputs written to BASE/results/:
#   pair_catalog_auto.csv
#   calls_all_pairs.csv
#   duplicate_report_by_pair.csv
#   Ineligibility_breakdown.csv
#   Table_all_pairs_by_species.csv
#   Fig1_all_pairs_by_species_FIXED.pdf
#   Fig2_*_logo.{pdf,png}, Fig2_*_entropy.{pdf,png}
#   Fig3_Ppalmivora_ITS6_rarefaction.{pdf,png,csv}
#   Delta_coverage_by_species_pair.csv
#   FigS_DeltaCoverage.pdf                (optional, enabled by flag below)
#   Table_S3_sensitivity.csv
#   FigS_Sensitivity.pdf                  (optional, enabled by flag below)
#
# Minimal edits: change BASE below to your local path.

from pathlib import Path
import sys, platform, datetime, re, random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
from Bio import SeqIO

# Optional: sequence logos (used if installed)
try:
    import logomaker as lm
    HAS_LOGOMAKER = True
except Exception:
    HAS_LOGOMAKER = False

# -------------------- user paths & toggles --------------------
BASE = Path(__file__)  # <- EDIT THIS PATH
CATALOG = BASE / "primer_catalog.csv"                         # columns: name,seq,dir
OUT = BASE / "results"
OUT.mkdir(parents=True, exist_ok=True)

# Your folders (a single .fasta file OR a directory containing .fa/.fasta files)
SPECIES_DIRS = {
    "M. perniciosa": BASE / "Mperniciosa.fasta",
    "M. roreri":     BASE / "Mroreri.fasta",
    "P. megakarya":  BASE / "Pmegakarya",
    "P. palmivora":  BASE / "Ppalmivora",
}

# Optional figures (Supplement)
MAKE_DELTA_FIG = True        # FigS_DeltaCoverage.pdf
MAKE_SENSITIVITY_FIG = True  # FigS_Sensitivity.pdf

# -------------------- reproducibility: write session info --------------------
def write_session_info(out_dir: Path):
    try:
        pkgs = {
            "python": sys.version.replace("\n", " "),
            "platform": platform.platform(),
            "numpy": np.__version__,
            "pandas": pd.__version__,
            "matplotlib": plt.matplotlib.__version__,
            "biopython": "available",
            "logomaker": (getattr(sys.modules.get("logomaker", None), "__version__", "not_installed")),
        }
        with open(out_dir / "SESSION_INFO.txt", "w", encoding="utf-8") as fh:
            fh.write(f"Run timestamp: {datetime.datetime.now().isoformat()}\n")
            for k, v in pkgs.items():
                fh.write(f"{k}: {v}\n")
    except Exception as e:
        print("WARN: could not write SESSION_INFO.txt:", e)

write_session_info(OUT)

# -------------------- DNA helpers --------------------
IUPAC = {
    "A":{"A"}, "C":{"C"}, "G":{"G"}, "T":{"T"}, "U":{"T"}, "N":{"A","C","G","T"},
    "R":{"A","G"}, "Y":{"C","T"}, "S":{"G","C"}, "W":{"A","T"},
    "K":{"G","T"}, "M":{"A","C"}, "B":{"C","G","T"}, "D":{"A","G","T"},
    "H":{"A","C","T"}, "V":{"A","C","G"},
}
comp = {"A":"T","C":"G","G":"C","T":"A","U":"A","R":"Y","Y":"R","S":"S","W":"W",
        "K":"M","M":"K","B":"V","D":"H","H":"D","V":"B","N":"N"}

def rc(s: str) -> str:
    return "".join(comp.get(b, "N") for b in s.upper())[::-1]

def iupac_match(q: str, r: str) -> bool:
    return r.upper() in IUPAC.get(q.upper(), {"N"})

def mismatch_count(primer: str, window: str) -> int:
    return sum(0 if iupac_match(p, w) else 1 for p, w in zip(primer, window))

def three_prime_mismatch(primer: str, window: str, k: int = 1) -> bool:
    """Return True if any mismatch in the 3' terminal k bases."""
    if k <= 0: return False
    return not all(iupac_match(p, w) for p, w in zip(primer[-k:], window[-k:]))

def scan_best(site_seq: str, primer: str, max_mm: int = 4):
    """Best match (pos, mm, window) on + strand of the string provided; None if all > max_mm."""
    L, best = len(primer), None
    for i in range(0, len(site_seq) - L + 1):
        w = site_seq[i:i+L]
        mm = mismatch_count(primer, w)
        if mm <= max_mm and (best is None or mm < best[1]):
            best = (i, mm, w)
            if mm == 0: break
    return best

# -------------------- FASTA loading --------------------
def clean_seq(s: str) -> str:
    s = s.upper().replace("U","T")
    return re.sub(r"[^ACGTN]", "N", s)

def load_species_records(dir_or_file: Path):
    """Return list of (accession, sequence) from either a directory of FASTAs or a single fasta file."""
    recs = []
    if dir_or_file.is_file() and dir_or_file.suffix.lower() in (".fa",".fasta",".fas",".fna"):
        for r in SeqIO.parse(str(dir_or_file), "fasta"):
            recs.append((r.id.split()[0], clean_seq(str(r.seq))))
        return recs
    for fp in sorted(dir_or_file.glob("*.fa*")):
        for r in SeqIO.parse(str(fp), "fasta"):
            recs.append((r.id.split()[0], clean_seq(str(r.seq))))
    return recs

species_to_records = {sp: load_species_records(path) for sp, path in SPECIES_DIRS.items()}
for sp, recs in species_to_records.items():
    print(f"{sp}: {len(recs)} accessions")

# -------------------- Primer catalog --------------------
# Expect columns: name, seq, dir   (dir = FORWARD/REVERSE or forward/reverse)
primers = pd.read_csv(CATALOG, comment="#").dropna(subset=["name","seq","dir"])
primers["name"] = primers["name"].astype(str).str.strip()
primers["seq"]  = primers["seq"].astype(str).str.strip().str.upper()
primers["dir"]  = primers["dir"].astype(str).str.strip().str.lower()

P = {row["name"]: {"seq": row["seq"], "dir": row["dir"]} for _, row in primers.iterrows()}
def have(*names): return all(n in P for n in names)

# -------------------- Pair catalog (align to manuscript/Table 2) --------------------
PAIR_DEFS = []
def add_pair(f, r, tmin, tmax, tag):
    """Register a pair only if both primers exist. Avoid accidental duplicates."""
    if have(f, r):
        PAIR_DEFS.append({
            "pair_name": f"{f}/{r}",
            "fwd_name": f, "rev_name": r,
            "fwd": P[f]["seq"], "rev": P[r]["seq"],
            "tmin": int(tmin), "tmax": int(tmax), "tag": tag
        })

# ITS1-sub (~250–400 bp) — fungal defaults
for f in ["ITS1","ITS1F","ITS5"]:
    for r in ["ITS2","ITS2_KYO2"]:
        add_pair(f, r, 250, 400, "ITS1_sub")

# Long, LSU-anchored classics (for completeness of the matrix)
add_pair("ITS1","ITS4", 800, 950, "long")
add_pair("ITS5","ITS4", 800, 950, "long")

# ITS2-sub (fungal) / long readouts (Monp-ITS4R shown in decision tree but not tallied)
add_pair("fITS7",     "ITS4", 250, 600, "ITS2_sub")
add_pair("ITS86F",    "ITS4", 250, 600, "ITS2_sub")
add_pair("ITS3_KYO2", "ITS4", 500, 700, "ITS2_sub")

# Oomycetes (short default in Results uses ITS1/ITS2_KYO2 for Phytophthora)
add_pair("ITS6","ITS4", 500, 950, "oomycete")
add_pair("ITS1","ITS2_KYO2", 250, 400, "oomycete_short_like")  # used as short oomycete assay

pairs = pd.DataFrame(PAIR_DEFS).drop_duplicates(
    subset=["pair_name","tmin","tmax","fwd","rev"]
).reset_index(drop=True)
pairs.to_csv(OUT / "pair_catalog_auto.csv", index=False)

# -------------------- Strict call engine --------------------
def call_pair_on_seq(seq, fwd, rev, tmin, tmax, strict_total_mm=2, max_mm_per_primer=4):
    """Return dict of strict rule outcomes for one (sequence, primer pair)."""
    f = scan_best(seq, fwd, max_mm=max_mm_per_primer)
    r_rc = scan_best(rc(seq), rev, max_mm=max_mm_per_primer)  # reverse primer on rc(seq)
    out = {"eligible":0, "strict_hit":0, "total_mm":None, "any_3p":0,
           "product_len_bp":None, "elig_reason":None}
    if not f and not r_rc: out["elig_reason"]="no_fwd_no_rev"; return out
    if not f:              out["elig_reason"]="no_fwd";        return out
    if not r_rc:           out["elig_reason"]="no_rev";        return out

    # map reverse site back to + strand start
    r_start_plus = len(seq) - (r_rc[0] + len(rev))
    prod_len = r_start_plus + len(rev) - f[0]
    if prod_len <= 0:
        out["elig_reason"]="wrong_orientation"; return out
    out["product_len_bp"] = prod_len
    if not (tmin <= prod_len <= tmax):
        out["elig_reason"]="outside_window"; return out

    # eligible -> apply strict rules
    out["eligible"] = 1
    total_mm = f[1] + r_rc[1]
    any3 = three_prime_mismatch(fwd, f[2], 1) or three_prime_mismatch(rev, r_rc[2], 1)
    out["total_mm"] = total_mm
    out["any_3p"] = int(any3)
    if (total_mm <= strict_total_mm) and (not any3):
        out["strict_hit"] = 1
    return out

# -------------------- Run calls (baseline strict rules) --------------------
rows = []
for sp, recs in species_to_records.items():
    for acc, seq in recs:
        for _, p in pairs.iterrows():
            c = call_pair_on_seq(seq, p["fwd"], p["rev"], p["tmin"], p["tmax"])
            rows.append({
                "species": sp, "accession": acc,
                "pair_name": p["pair_name"], "tag": p["tag"],
                "fwd_name": p["fwd_name"], "rev_name": p["rev_name"],
                **c
            })

calls = pd.DataFrame(rows)
calls.to_csv(OUT / "calls_all_pairs.csv", index=False)

# -------------------- Duplication detection & prevention --------------------
dup_matrix = (calls.groupby(["species","pair_name","accession"])
                   .size().reset_index(name="rows_per_accession"))
dup_summary = (dup_matrix.groupby(["species","pair_name"], as_index=False)
                         .agg(n_accessions=("accession","nunique"),
                              total_rows=("rows_per_accession","sum"),
                              max_rows_per_accession=("rows_per_accession","max")))
dup_summary["has_duplicates"] = dup_summary["total_rows"] > dup_summary["n_accessions"]
dup_summary["duplication_ratio"] = (dup_summary["total_rows"] / dup_summary["n_accessions"]).round(3)
dup_summary.to_csv(OUT / "duplicate_report_by_pair.csv", index=False)

# -------------------- Collapse to ONE row per accession×pair_name --------------------
clean = (calls.groupby(["species","pair_name","accession"], as_index=False)
               .agg(strict_hit=("strict_hit","max"),
                    eligible=("eligible","max")))

# -------------------- Deduplicated ineligibility reasons --------------------
def pick_reason(sub: pd.DataFrame) -> str:
    if int(sub["eligible"].max()) == 1:
        return ""  # eligible at least once; not counted as ineligible
    order = ["no_rev","no_fwd","no_fwd_no_rev","wrong_orientation","outside_window"]
    reasons = [r for r in sub["elig_reason"].dropna().tolist() if r]
    for key in order:
        if key in reasons:
            return key
    return reasons[0] if reasons else "unknown"

inelig_rows = []
for (sp, pair, acc), sub in calls.groupby(["species","pair_name","accession"]):
    reason = pick_reason(sub)
    if reason:
        inelig_rows.append({"species":sp,"pair_name":pair,"accession":acc,"reason":reason})
inelig_df = pd.DataFrame(inelig_rows)
inelig_breakdown = (inelig_df.groupby(["species","pair_name","reason"], as_index=False)
                             .size().rename(columns={"size":"count"}))
inelig_breakdown.to_csv(OUT / "Ineligibility_breakdown.csv", index=False)

# -------------------- Species-level tallies (deduplicated) --------------------
summ = (clean.groupby(["species","pair_name"], as_index=False)
             .agg(N=("accession","nunique"),
                  covered=("strict_hit","sum"),
                  eligible=("eligible","sum")))
summ["eligible_not_strict"] = summ["eligible"] - summ["covered"]
summ["ineligible"] = summ["N"] - summ["eligible"]
summ["covered_frac"] = (summ["covered"] / summ["N"]).fillna(0.0)
summ["eligible_not_strict_frac"] = (summ["eligible_not_strict"] / summ["N"]).fillna(0.0)
summ["ineligible_frac"] = (summ["ineligible"] / summ["N"]).fillna(0.0)
# Coverage among eligibles (%), as used in Table 2 of the manuscript
summ["covered_pct_eligible"] = np.where(
    summ["eligible"] > 0,
    (summ["covered"] / summ["eligible"] * 100).round(1),
    np.nan
)
summ.to_csv(OUT / "Table_all_pairs_by_species.csv", index=False)

# -------------------- Figure 1: coverage (multi-page; one page per species) --------------------
def italicize(sp_short: str) -> str:
    g = sp_short.split(".")[0].strip()
    s = sp_short.split(".")[1].strip()
    return rf"$\it{{{g}.\ {s}}}$"

def plot_species_page(sdata: pd.DataFrame, species: str, pdf: PdfPages):
    d = sdata[sdata["species"]==species].copy()
    if d.empty: return
    d = d.sort_values("pair_name")

    labels = d["pair_name"].tolist()
    covered = d["covered_frac"].to_numpy()
    elig_not = d["eligible_not_strict_frac"].to_numpy()
    inelig = d["ineligible_frac"].to_numpy()
    N_species = int(d["N"].max())

    fig_w = max(10, 0.55*len(labels))
    fig, ax = plt.subplots(figsize=(fig_w, 6))
    x = np.arange(len(labels))
    ax.bar(x, covered, label="Covered (strict)")
    ax.bar(x, elig_not, bottom=covered, label="Eligible, not strict")
    ax.bar(x, inelig, bottom=covered+elig_not, label="Ineligible")

    ax.set_ylim(0, 1.10)
    ax.set_ylabel("Fraction of all accessions")
    ax.set_xticks(x, [ "\n".join(re.findall('.{1,16}', l)) for l in labels ],
                  rotation=45, ha="right", fontsize=9)
    for xi in x:
        ax.text(xi, 1.02, f"N={N_species}", ha="center", va="bottom", rotation=90, fontsize=8)

    ax.legend(loc="upper center", bbox_to_anchor=(0.5, 1.12), ncol=3, frameon=False)
    ax.set_title(italicize(species), y=1.22, fontsize=14)

    fig.tight_layout(); fig.subplots_adjust(top=0.78)
    pdf.savefig(fig, dpi=300); plt.close(fig)

pdf_path = OUT / "Fig1_all_pairs_by_species_FIXED.pdf"
with PdfPages(pdf_path) as pdf:
    for sp in ["M. perniciosa","M. roreri","P. megakarya","P. palmivora"]:
        plot_species_page(summ, sp, pdf)
print("Wrote", pdf_path)

# -------------------- Binding-site logos & entropy --------------------
def collect_windows(records, primer_seq, max_mm_find=8, search_on_rc=False):
    wins = []
    for acc, seq in records:
        s = rc(seq) if search_on_rc else seq
        hit = scan_best(s, primer_seq, max_mm=max_mm_find)
        if hit: wins.append(hit[2].replace("U","T"))
    return wins

def logo_and_entropy(windows, title, out_prefix):
    if not windows:
        print("No windows for", title); return
    L = len(windows[0])
    counts = {b:[0]*L for b in "ACGT"}
    for w in windows:
        for i,ch in enumerate(w):
            if ch in counts: counts[ch][i] += 1
    mat = pd.DataFrame(counts, columns=list("ACGT"))
    probs = mat.div(mat.sum(axis=1), axis=0).fillna(0.0)

    # Shannon entropy per position (bits), information content = 2 - H (DNA)
    with np.errstate(divide='ignore', invalid='ignore'):
        ent = - (probs.replace(0, np.nan) * np.log2(probs.replace(0, np.nan))).sum(axis=1)
    ent = ent.fillna(0.0)
    info = (2.0 - ent).clip(lower=0.0, upper=2.0)  # DNA: max 2 bits

    # Logo (information content in bits)
    if HAS_LOGOMAKER:
        heights = probs.mul(info, axis=0)  # scale letter heights by information content
        fig = plt.figure(figsize=(8,2.6)); ax = plt.gca()
        lm.Logo(heights, ax=ax); ax.axvline(L-1, ls="--", lw=1)
        ax.set_title(f"{title} (n={len(windows)})")
        ax.set_xlabel("Position 5'→3'"); ax.set_ylabel("Bits")
        fig.tight_layout()
        fig.savefig(OUT/f"{out_prefix}_logo.pdf"); fig.savefig(OUT/f"{out_prefix}_logo.png", dpi=220)
        plt.close(fig)
    else:
        # Fallback: plot base probabilities
        fig = plt.figure(figsize=(8,2.6)); ax = plt.gca()
        for b in "ACGT":
            ax.plot(range(1,L+1), probs[b].to_numpy(), marker="o", label=b)
        ax.legend(); ax.axvline(L-1, ls="--", lw=1)
        ax.set_title(f"{title} (n={len(windows)})")
        ax.set_xlabel("Position 5'→3'"); ax.set_ylabel("Probability")
        fig.tight_layout()
        fig.savefig(OUT/f"{out_prefix}_logo.pdf"); fig.savefig(OUT/f"{out_prefix}_logo.png", dpi=220)
        plt.close(fig)

    # Entropy curve (bits)
    fig = plt.figure(figsize=(8,2.2)); ax = plt.gca()
    ax.plot(range(1,L+1), ent.values)
    ax.axvline(L-1, ls="--", lw=1)
    ax.set_title(f"{title} • entropy"); ax.set_xlabel("Position 5'→3'"); ax.set_ylabel("Shannon (bits)")
    fig.tight_layout()
    fig.savefig(OUT/f"{out_prefix}_entropy.pdf"); fig.savefig(OUT/f"{out_prefix}_entropy.png", dpi=220)
    plt.close(fig)

# Examples used in manuscript
wins = collect_windows(species_to_records["M. perniciosa"], P["fITS7"]["seq"])
logo_and_entropy(wins, "Moniliophthora perniciosa • fITS7 binding site", "Fig2_Mperniciosa_fITS7")

wins = collect_windows(species_to_records["P. palmivora"], P["ITS6"]["seq"])
logo_and_entropy(wins, "P. palmivora • ITS6 binding site", "Fig2_Ppalmivora_ITS6")

wins = collect_windows(species_to_records["P. palmivora"], P["ITS4"]["seq"], search_on_rc=True)
logo_and_entropy(wins, "P. palmivora • ITS4 binding site", "Fig2_Ppalmivora_ITS4")

wins = collect_windows(species_to_records["P. palmivora"], P["ITS3_KYO2"]["seq"])
logo_and_entropy(wins, "P. palmivora • ITS3_KYO2 binding site", "Fig2_Ppalmivora_ITS3KYO2")

# -------------------- Rarefaction (ITS6 in P. palmivora) --------------------
def rarefy(windows, reps=50, seed=17):
    if not windows: return None, None, 0
    random.seed(seed); n=len(windows)
    arr = np.zeros((reps, n), float)
    for r in range(reps):
        order = windows[:]; random.shuffle(order)
        seen=set(); c=[]
        for w in order:
            seen.add(w); c.append(len(seen))
        arr[r,:] = c
    return arr.mean(0), arr, len(set(windows))

def plot_rarefaction(mean_curve, total_unique, title, out_prefix):
    if mean_curve is None: 
        print("No windows to rarefy for", title); return
    plt.figure(figsize=(5.8,3.2))
    plt.plot(range(1,len(mean_curve)+1), mean_curve)
    plt.axhline(0.95*total_unique, ls="--")
    plt.xlabel("Number of sequences sampled"); plt.ylabel("Unique binding-site haplotypes")
    plt.title(title)
    plt.tight_layout()
    plt.savefig(OUT/f"{out_prefix}_rarefaction.pdf")
    plt.savefig(OUT/f"{out_prefix}_rarefaction.png", dpi=220)
    plt.close()
    pd.DataFrame({"n_sequences": np.arange(1, len(mean_curve)+1),
                  "mean_unique": mean_curve}
                ).to_csv(OUT/f"{out_prefix}_rarefaction.csv", index=False)

pp_wins = collect_windows(species_to_records["P. palmivora"], P["ITS6"]["seq"])
mean_curve, _, total_unique = rarefy(pp_wins, reps=50, seed=17)
plot_rarefaction(mean_curve, total_unique,
                 "P. palmivora • ITS6 binding-site rarefaction",
                 "Fig3_Ppalmivora_ITS6")

if mean_curve is not None:
    n95 = next((i+1 for i,v in enumerate(mean_curve) if v >= 0.95*total_unique), len(mean_curve))
    print(f"ITS6 unique haplotypes = {total_unique}; 95% reached by ~{n95} sequences.")

# -------------------- NEW: Δ‑coverage (naïve vs eligibility‑aware) --------------------
dc = summ.copy()
dc["naive_cov"] = (dc["covered"] / dc["N"]).round(4)  # covered / all
dc["elig_cov"]  = (dc["covered"] / dc["eligible"]).replace([np.inf, np.nan], 0).round(4)  # covered / eligible
dc["delta_cov"] = (dc["elig_cov"] - dc["naive_cov"]).round(4)
dc = dc[["species","pair_name","N","eligible","covered","naive_cov","elig_cov","delta_cov"]]
dc.to_csv(OUT/"Delta_coverage_by_species_pair.csv", index=False)
print("Wrote", OUT/"Delta_coverage_by_species_pair.csv")

if MAKE_DELTA_FIG:
    pdf_delta = OUT / "FigS_DeltaCoverage.pdf"
    with PdfPages(pdf_delta) as pdf:
        for sp in sorted(dc["species"].unique()):
            d = dc[dc["species"]==sp].sort_values("pair_name")
            x = np.arange(len(d))
            fig, ax = plt.subplots(figsize=(max(8, 0.7*len(d)), 4.2))
            ax.bar(x-0.18, d["naive_cov"].values, width=0.36, label="Naïve (covered/N)")
            ax.bar(x+0.18, d["elig_cov"].values,  width=0.36, label="Eligible (covered/eligible)")
            for xi, dv in enumerate(d["delta_cov"].values):
                ax.text(xi, max(d["naive_cov"].iloc[xi], d["elig_cov"].iloc[xi]) + 0.02,
                        f"Δ={dv:.2f}", ha="center", va="bottom", fontsize=8)
            ax.set_xticks(x, d["pair_name"], rotation=45, ha="right")
            ax.set_ylim(0, min(1.05, (max(d["naive_cov"].max(), d["elig_cov"].max())+0.12)))
            ax.set_ylabel("Coverage")
            ax.set_title(f"{sp}: naïve vs eligibility‑aware")
            ax.legend(frameon=False)
            fig.tight_layout(); pdf.savefig(fig, dpi=300); plt.close(fig)
    print("Wrote", pdf_delta)

# -------------------- NEW: Sensitivity analysis tables/figure --------------------
def recompute_summary(strict_total_mm=2, allow_3prime=False):
    """Recompute eligibility-aware coverage under alternative scoring rules."""
    rows2=[]
    for sp, recs in species_to_records.items():
        for acc, seq in recs:
            for _, p in pairs.iterrows():
                f = scan_best(seq, p["fwd"], max_mm=4)
                r_rc = scan_best(rc(seq), p["rev"], max_mm=4)
                eligible=0; total_mm=None; any3=0; strict=0
                if f and r_rc:
                    # orientation & window
                    r_start_plus = len(seq) - (r_rc[0] + len(p["rev"]))
                    prod_len = r_start_plus + len(p["rev"]) - f[0]
                    if (prod_len > 0) and (p["tmin"] <= prod_len <= p["tmax"]):
                        eligible=1
                        total_mm = f[1] + r_rc[1]
                        any3 = int(three_prime_mismatch(p["fwd"], f[2], 1) or
                                   three_prime_mismatch(p["rev"], r_rc[2], 1))
                        if (total_mm is not None) and (total_mm <= strict_total_mm):
                            if allow_3prime or (any3==0):
                                strict=1
                rows2.append({"species":sp,"pair_name":p["pair_name"],"accession":acc,
                              "eligible":eligible,"strict_hit":strict})
    df = pd.DataFrame(rows2)
    sm = (df.groupby(["species","pair_name"], as_index=False)
            .agg(N=("accession","nunique"),
                 covered=("strict_hit","sum"),
                 eligible=("eligible","sum")))
    sm["elig_cov"] = (sm["covered"]/sm["eligible"]).replace([np.inf,np.nan],0.0)
    return sm

base = summ[["species","pair_name","eligible","covered"]].copy()
base["elig_cov_base"] = (base["covered"]/base["eligible"]).replace([np.inf,np.nan],0.0)

mm3  = recompute_summary(strict_total_mm=3, allow_3prime=False)
mm3  = mm3.rename(columns={"elig_cov":"elig_cov_mm3"}).drop(columns=["N","covered","eligible"])

allow3 = recompute_summary(strict_total_mm=2, allow_3prime=True)
allow3 = allow3.rename(columns={"elig_cov":"elig_cov_3prime"}).drop(columns=["N","covered","eligible"])

sens = (base.merge(mm3,  on=["species","pair_name"])
             .merge(allow3, on=["species","pair_name"]))
sens["delta_mm3"]    = (sens["elig_cov_mm3"]   - sens["elig_cov_base"]).round(4)
sens["delta_3prime"] = (sens["elig_cov_3prime"]- sens["elig_cov_base"]).round(4)
sens_out = sens[["species","pair_name","elig_cov_base","elig_cov_mm3","delta_mm3",
                 "elig_cov_3prime","delta_3prime"]]
sens_out.to_csv(OUT/"Table_S3_sensitivity.csv", index=False)
print("Wrote", OUT/"Table_S3_sensitivity.csv")

if MAKE_SENSITIVITY_FIG:
    # Small panel showing deltas for each species × pair
    fig, ax = plt.subplots(figsize=(7.5, 5.0))
    ax.scatter(sens["delta_mm3"], sens["delta_3prime"], alpha=0.7)
    ax.axvline(0, color="k", lw=0.5); ax.axhline(0, color="k", lw=0.5)
    ax.set_xlabel("Δ coverage (≤3 mismatches minus baseline)")
    ax.set_ylabel("Δ coverage (allow 3′ mismatch minus baseline)")
    ax.set_title("Sensitivity analysis across species × pairs")
    fig.tight_layout()
    fig.savefig(OUT/"FigS_Sensitivity.pdf", dpi=300)
    plt.close(fig)
    print("Wrote", OUT/"FigS_Sensitivity.pdf")

print("DONE. CSVs and figures in:", OUT)

