from pathlib import Path
import sys, platform, datetime, re
import numpy as np
import pandas as pd
from Bio import SeqIO

# -------------------- user paths & toggles --------------------
BASE = Path(__file__).parent  # Points to directory containing the script
CATALOG = BASE / "data" / "primer_catalog.csv"  # Updated path to catalog
OUT = BASE / "results"
OUT.mkdir(parents=True, exist_ok=True)

# Your folders (a single .fasta file OR a directory containing .fa/.fasta files)
SPECIES_DIRS = {
    "M. perniciosa": BASE / "data" / "Mperniciosa.fasta",
    "M. roreri":     BASE / "data" / "Mroreri.fasta",
    "P. megakarya":  BASE / "data" / "Pmegakarya",
    "P. palmivora":  BASE / "data" / "Ppalmivora",
}

# -------------------- DNA helpers --------------------
IUPAC = {
    "A":{"A"}, "C":{"C"}, "G":{"G"}, "T":{"T"}, "U":{"T"}, "N":{"A","C","G","T"},
    "R":{"A","G"}, "Y":{"C","T"}, "S":{"G","C"}, "W":{"A","T"},
    "K":{"G","T"}, "M":{"A","C"}, "B":{"C","G","T"}, "D":{"A","G","T"},
    "H":{"A","C","T"}, "V":{"A","C","G"},
}
comp = {"A":"T","C":"G","G":"C","T":"A","U":"A","R":"Y","Y":"R","S":"S","W":"W",
        "K":"M","M":"K","B":"V","D":"H","H":"D","V":"B","N":"N"}

def rc(s: str) -> str:
    return "".join(comp.get(b, "N") for b in s.upper())[::-1]

def iupac_match(q: str, r: str) -> bool:
    return r.upper() in IUPAC.get(q.upper(), {"N"})

def mismatch_count(primer: str, window: str) -> int:
    return sum(0 if iupac_match(p, w) else 1 for p, w in zip(primer, window))

def three_prime_mismatch(primer: str, window: str, k: int = 1) -> bool:
    """Return True if any mismatch in the 3' terminal k bases."""
    if k <= 0: return False
    return not all(iupac_match(p, w) for p, w in zip(primer[-k:], window[-k:]))

def scan_best(site_seq: str, primer: str, max_mm: int = 4):
    """Best match (pos, mm, window) on + strand of the string provided; None if all > max_mm."""
    L, best = len(primer), None
    for i in range(0, len(site_seq) - L + 1):
        w = site_seq[i:i+L]
        mm = mismatch_count(primer, w)
        if mm <= max_mm and (best is None or mm < best[1]):
            best = (i, mm, w)
            if mm == 0: break
    return best

# -------------------- FASTA loading --------------------
def clean_seq(s: str) -> str:
    s = s.upper().replace("U","T")
    return re.sub(r"[^ACGTN]", "N", s)

def load_species_records(dir_or_file: Path):
    """Return list of (accession, sequence) from either a directory of FASTAs or a single fasta file."""
    recs = []
    if dir_or_file.is_file() and dir_or_file.suffix.lower() in (".fa",".fasta",".fas",".fna"):
        for r in SeqIO.parse(str(dir_or_file), "fasta"):
            recs.append((r.id.split()[0], clean_seq(str(r.seq))))
        return recs
    for fp in sorted(dir_or_file.glob("*.fa*")):
        for r in SeqIO.parse(str(fp), "fasta"):
            recs.append((r.id.split()[0], clean_seq(str(r.seq))))
    return recs

species_to_records = {sp: load_species_records(path) for sp, path in SPECIES_DIRS.items()}
for sp, recs in species_to_records.items():
    print(f"{sp}: {len(recs)} accessions")

# -------------------- Primer catalog --------------------
# Expect columns: name, seq, dir   (dir = FORWARD/REVERSE or forward/reverse)
primers = pd.read_csv(CATALOG, comment="#").dropna(subset=["name","seq","dir"])
primers["name"] = primers["name"].astype(str).str.strip()
primers["seq"]  = primers["seq"].astype(str).str.strip().str.upper()
primers["dir"]  = primers["dir"].astype(str).str.strip().str.lower()

P = {row["name"]: {"seq": row["seq"], "dir": row["dir"]} for _, row in primers.iterrows()}
def have(*names): return all(n in P for n in names)

# -------------------- Pair catalog (align to manuscript/Table 2) --------------------
PAIR_DEFS = []
def add_pair(f, r, tmin, tmax, tag):
    """Register a pair only if both primers exist. Avoid accidental duplicates."""
    if have(f, r):
        PAIR_DEFS.append({
            "pair_name": f"{f}/{r}",
            "fwd_name": f, "rev_name": r,
            "fwd": P[f]["seq"], "rev": P[r]["seq"],
            "tmin": int(tmin), "tmax": int(tmax), "tag": tag
        })

# ITS1-sub (~250–400 bp) — fungal defaults
for f in ["ITS1","ITS1F","ITS5"]:
    for r in ["ITS2","ITS2_KYO2"]:
        add_pair(f, r, 250, 400, "ITS1_sub")

# Long, LSU-anchored classics (for completeness of the matrix)
add_pair("ITS1","ITS4", 800, 950, "long")
add_pair("ITS5","ITS4", 800, 950, "long")

# ITS2-sub (fungal) / long readouts (Monp-ITS4R shown in decision tree but not tallied)
add_pair("fITS7",     "ITS4", 250, 600, "ITS2_sub")
add_pair("ITS86F",    "ITS4", 250, 600, "ITS2_sub")
add_pair("ITS3_KYO2", "ITS4", 500, 700, "ITS2_sub")

# Oomycetes (short default in Results uses ITS1/ITS2_KYO2 for Phytophthora)
add_pair("ITS6","ITS4", 500, 950, "oomycete")
add_pair("ITS1","ITS2_KYO2", 250, 400, "oomycete_short_like")  # used as short oomycete assay

pairs = pd.DataFrame(PAIR_DEFS).drop_duplicates(
    subset=["pair_name","tmin","tmax","fwd","rev"]
).reset_index(drop=True)
pairs.to_csv(OUT / "pair_catalog_auto.csv", index=False)

# -------------------- Strict call engine --------------------
def call_pair_on_seq(seq, fwd, rev, tmin, tmax, strict_total_mm=2, max_mm_per_primer=4):
    """Return dict of strict rule outcomes for one (sequence, primer pair)."""
    f = scan_best(seq, fwd, max_mm=max_mm_per_primer)
    r_rc = scan_best(rc(seq), rev, max_mm=max_mm_per_primer)  # reverse primer on rc(seq)
    out = {"eligible":0, "strict_hit":0, "total_mm":None, "any_3p":0,
           "product_len_bp":None, "elig_reason":None}
    if not f and not r_rc: out["elig_reason"]="no_fwd_no_rev"; return out
    if not f:              out["elig_reason"]="no_fwd";        return out
    if not r_rc:           out["elig_reason"]="no_rev";        return out

    # map reverse site back to + strand start
    r_start_plus = len(seq) - (r_rc[0] + len(rev))
    prod_len = r_start_plus + len(rev) - f[0]
    if prod_len <= 0:
        out["elig_reason"]="wrong_orientation"; return out
    out["product_len_bp"] = prod_len
    if not (tmin <= prod_len <= tmax):
        out["elig_reason"]="outside_window"; return out

    # eligible -> apply strict rules
    out["eligible"] = 1
    total_mm = f[1] + r_rc[1]
    any3 = three_prime_mismatch(fwd, f[2], 1) or three_prime_mismatch(rev, r_rc[2], 1)
    out["total_mm"] = total_mm
    out["any_3p"] = int(any3)
    if (total_mm <= strict_total_mm) and (not any3):
        out["strict_hit"] = 1
    return out

# -------------------- Run calls (baseline strict rules) --------------------
rows = []
for sp, recs in species_to_records.items():
    for acc, seq in recs:
        for _, p in pairs.iterrows():
            c = call_pair_on_seq(seq, p["fwd"], p["rev"], p["tmin"], p["tmax"])
            rows.append({
                "species": sp, "accession": acc,
                "pair_name": p["pair_name"], "tag": p["tag"],
                "fwd_name": p["fwd_name"], "rev_name": p["rev_name"],
                **c
            })

calls = pd.DataFrame(rows)
calls.to_csv(OUT / "calls_all_pairs.csv", index=False)

# -------------------- Duplication detection & prevention --------------------
dup_matrix = (calls.groupby(["species","pair_name","accession"])
                   .size().reset_index(name="rows_per_accession"))
dup_summary = (dup_matrix.groupby(["species","pair_name"], as_index=False)
                         .agg(n_accessions=("accession","nunique"),
                              total_rows=("rows_per_accession","sum"),
                              max_rows_per_accession=("rows_per_accession","max")))
dup_summary["has_duplicates"] = dup_summary["total_rows"] > dup_summary["n_accessions"]
dup_summary["duplication_ratio"] = (dup_summary["total_rows"] / dup_summary["n_accessions"]).round(3)
dup_summary.to_csv(OUT / "duplicate_report_by_pair.csv", index=False)

# -------------------- Collapse to ONE row per accession×pair_name --------------------
clean = (calls.groupby(["species","pair_name","accession"], as_index=False)
               .agg(strict_hit=("strict_hit","max"),
                    eligible=("eligible","max")))

# -------------------- Deduplicated ineligibility reasons --------------------
def pick_reason(sub: pd.DataFrame) -> str:
    if int(sub["eligible"].max()) == 1:
        return ""  # eligible at least once; not counted as ineligible
    order = ["no_rev","no_fwd","no_fwd_no_rev","wrong_orientation","outside_window"]
    reasons = [r for r in sub["elig_reason"].dropna().tolist() if r]
    for key in order:
        if key in reasons:
            return key
    return reasons[0] if reasons else "unknown"

inelig_rows = []
for (sp, pair, acc), sub in calls.groupby(["species","pair_name","accession"]):
    reason = pick_reason(sub)
    if reason:
        inelig_rows.append({"species":sp,"pair_name":pair,"accession":acc,"reason":reason})
inelig_df = pd.DataFrame(inelig_rows)
inelig_breakdown = (inelig_df.groupby(["species","pair_name","reason"], as_index=False)
                             .size().rename(columns={"size":"count"}))
inelig_breakdown.to_csv(OUT / "Ineligibility_breakdown.csv", index=False)

# -------------------- Species-level tallies (deduplicated) --------------------
summ = (clean.groupby(["species","pair_name"], as_index=False)
             .agg(N=("accession","nunique"),
                  covered=("strict_hit","sum"),
                  eligible=("eligible","sum")))
summ["eligible_not_strict"] = summ["eligible"] - summ["covered"]
summ["ineligible"] = summ["N"] - summ["eligible"]
summ["covered_frac"] = (summ["covered"] / summ["N"]).fillna(0.0)
summ["eligible_not_strict_frac"] = (summ["eligible_not_strict"] / summ["N"]).fillna(0.0)
summ["ineligible_frac"] = (summ["ineligible"] / summ["N"]).fillna(0.0)
# Coverage among eligibles (%), as used in Table 2 of the manuscript
summ["covered_pct_eligible"] = np.where(
    summ["eligible"] > 0,
    (summ["covered"] / summ["eligible"] * 100).round(1),
    np.nan
)
summ.to_csv(OUT / "Table_all_pairs_by_species.csv", index=False)

# -------------------- Δ‑coverage (naïve vs eligibility‑aware) --------------------
dc = summ.copy()
dc["naive_cov"] = (dc["covered"] / dc["N"]).round(4)  # covered / all
dc["elig_cov"]  = (dc["covered"] / dc["eligible"]).replace([np.inf, np.nan], 0).round(4)  # covered / eligible
dc["delta_cov"] = (dc["elig_cov"] - dc["naive_cov"]).round(4)
dc = dc[["species","pair_name","N","eligible","covered","naive_cov","elig_cov","delta_cov"]]
dc.to_csv(OUT/"Delta_coverage_by_species_pair.csv", index=False)
print("Wrote", OUT/"Delta_coverage_by_species_pair.csv")

print("DONE. CSVs in:", OUT)